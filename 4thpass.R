
#also doing another pass after this finalList creating a finalListCV
#PCA Analysis, scratch space post analysis, currently need to do classification matrix.  would recommend doing it on samples?  
#Also derive population stuff here
#no need for randomized sets (unless validating, but as long as what is produced is significant each time shown, then the experiment is a success)


#due to way NA's are presented, there is a deviation in the # of records truly presented... but unsure if since na's are represented evenly if this matters or not.
#What I might need to do is remove na's from newDF
files <- list.files(path=paste0(sourceDir,'/output/'), pattern="*.csv", full.names=TRUE, recursive=FALSE)

for (postProcess in 1:length(files))
{ 
  
  #NewDF assumes 0's mean NA's, this is more like a population dataframe already precleaned.
  PostDF <- read.csv(files[postProcess], header=TRUE, sep=",")[,-1]
  
  trainModel <- suppressMessages(train(PostDF[-1], as.factor(PostDF[,1]),method = "glm",trControl = train.control))
  print("population")
  print(summary(trainModel$finalModel))
  
  #reseed
  source(paste0(sourceDir,"/reseedPost.R"))
  source(paste0(sourceDir,"/resampleMCpost.R"))
  
  res <- cor(data.train)
  corrplot(res)
  
  x= c()
  y= c()
  #yname <- c()
  
  x=data.train[,-1]
  y=data.train[,1]
  
  pc <- prcomp(data.train[,-1], center=TRUE, scale=TRUE)
  
  #includes proportion of variance
  summary(prcomp(data.train[,-1], center=TRUE, scale=TRUE))
  te <- summary(prcomp(data.train[,-1], center=TRUE, scale=TRUE))$importance
  #pc plot
  plot(te[3,1:ncol(te)])
  
  #correlation plot of sample
  corrplot(cor(cbind(data.train[,1],prcomp(data.train[,-1], center=TRUE, scale=TRUE)$x)))
  
  #include data in new model for inclusion in a linear model
  #https://stats.stackexchange.com/questions/72839/how-to-use-r-prcomp-results-for-prediction
  
  suppressMessages(pcaModel<- glm(y~pc$x[,1:length(data.frame(pc$x))]))
  
  #predict using pca, just re-applying to training data.
  
  #applied PCA to holdout
  
  x <- data.test[,-1, drop=FALSE]
  
  y <- data.frame(data.test[,1, drop=FALSE])
  
  #does this make it linear?
  pred <- predict(pc,x)
  #plot(data.frame(y,drop=FALSE),pred)
  pcaPred <- lm(cbind(y,pred))
  
  #predict(pcaPred,)
  
  #predict(pcaPred,filteredv7133holdout[-1])
  
  #summary(pcaPred)
  hist(abs(pcaPred$residuals))
  
  summary(pcaModel)
  #summary(pcaPred)
  
  regularTrainModel <- suppressMessages(glm(data.train))
  regularTestModel <- suppressMessages(glm(data.test))
  
  # Define training control
  
  #http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/#k-fold-cross-validation
  #yname <- colnames(data.train[,1,drop=FALSE])
  #nrow(y)
  x=data.train[,-1,drop=FALSE]
  y=(data.train[,1,drop=FALSE])
  
  #View(data.train[,1])
  #will this work, train on train partition, and validate on a test partition?  Probably a bad idea, because I'm going to predict using test...
  trainModel <- suppressMessages(train(data.train[-1], as.factor(data.train[,1]),method = "glm",trControl = train.control))
  testModel <- suppressMessages(train(data.test[-1], as.factor(data.test[,1]), method = "glm",trControl = train.control))
  
  print("sig 1")
  print(summary(trainModel))
  
  print("sig 2")
  print(summary(testModel))
  
  holderOfData.train <- cbind(data.frame(data.train[,-1 , drop = FALSE]),data.frame(data.train[,1 , drop = FALSE]))
  holderOfData.test <- cbind(data.frame(data.test[,-1 , drop = FALSE]),data.frame(data.test[,1 , drop = FALSE]))
  
  if (widthDiviser==1)  A <- bestglm(Xy = holderOfData.train, IC="CV", CVArgs=list(Method="HTF", K=2, REP=widthDiviser, TopModels=widthDiviser, BestModels = widthDiviser), family=binomial,method = "exhaustive")
  if (widthDiviser!=1)  A <- bestglm(Xy = holderOfData.train, IC="CV", CVArgs=list(Method="HTF", K=widthDiviser, REP=widthDiviser, TopModels=widthDiviser, BestModels = widthDiviser), family=binomial,method = "exhaustive")
  print (A$Subsets)
  if (widthDiviser==1)  B <- bestglm(Xy = holderOfData.test, IC="CV", CVArgs=list(Method="HTF", K=2, REP=widthDiviser, TopModels=widthDiviser, BestModels = widthDiviser), family=binomial,method = "exhaustive")
  if (widthDiviser!=1)  B <- bestglm(Xy = holderOfData.test, IC="CV", CVArgs=list(Method="HTF", K=widthDiviser, REP=widthDiviser, TopModels=widthDiviser, BestModels = widthDiviser), family=binomial,method = "exhaustive")
  print(B$Subsets)
  
  merged <- rbind(data.train,data.test)
  holderOfData <- cbind(data.frame(merged[,-1 , drop = FALSE]),data.frame(merged[,1 , drop = FALSE]))
  
  #filter min
  #finalListCV <- sub_returnCVNamesExclMin(merged)
  #print(c("4: ", finalListCV))
  
  holderOfData <- cbind(data.frame(merged[,-1 , drop = FALSE]),data.frame(merged[,1 , drop = FALSE]))
  
  B <- bestglm(Xy = holderOfData, IC="CV", CVArgs=list(Method="HTF", K=2, REP=widthDiviser, TopModels=widthDiviser, BestModels = widthDiviser), family=binomial,method = "exhaustive")
  print(B$Subsets)
  
  trainModel <- c()
  trainModel <- suppressMessages( train(merged[, -1, drop = FALSE], as.factor(merged[,1]),method = "glm",trControl = train.control) )
  print("comb sig")
  print(summary(trainModel))
  #I swear I was doing predicitons before with better accuracy
  
  #reseed
  source(paste0(sourceDir,"/reseedPost.R"))
  source(paste0(sourceDir,"/resampleMCpost.R"))
  
  #test against new partitions
  #colnames(data.train)
  merged <- rbind(data.train,data.test)
  
  testModel <- c()
  testModelPred <- c()
  testModel <- suppressMessages(train(merged[,-1, drop = FALSE], as.factor(merged[,1]),method = "glm",trControl = train.control))
  
  #using newly acquired merged data, and prior trained model, derive predictions
  trainModelPred <- round(predict.glm(trainModel$finalModel, merged))
  print("test 2")
  print(summary(testModel$finalModel))
  hist(abs(trainModelPred-merged[,1]))
  
  #http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation
  #https://www.theanalysisfactor.com/r-tutorial-glm1/
}

